# LLM Code Comparison â€“ Gemini 2.5 Flash vs LLaMA-3.3-70B (Groq) 

This project allows you to compare two Large Language Models (LLMs) â€” **Google Gemini 2.5 Flash** and **Meta LLaMA-3.3-70B** (via Groq) â€” on their ability to generate source code for different programming tasks.

The system provides a **complete automated pipeline**:

1. You write a task/problem description  
2. Both LLMs generate Python code for the task  
3. The generated code is automatically executed  
4. The system evaluates the code using various metrics  
5. A Streamlit dashboard displays everything neatly  

ğŸš€

## Check out Demo Screenshots [Large Language Models Code Comparison Dashboard (Prototype)](https://mshazilkhandevs.vercel.app/projects/large-language-models-code-comparison-dashboard)

## What This Project Does

| Feature                                 | Description                                                                                                   |
|-----------------------------------------|---------------------------------------------------------------------------------------------------------------|
| âœ… **Create custom coding tasks**         | Write your own problem statements (e.g., â€œsort a listâ€, â€œfind prime numbersâ€, etc.). Each task is saved as a `.txt` file inside the `tasks/` folder. |
| âœ… **Auto-generate code using two LLMs** | Sends each task prompt to Gemini and LLaMA (Groq) and saves the outputs (e.g., `outputs/task_1/gemini.py`, `outputs/task_1/llama.py`). |
| âœ… **Run evaluation automatically**      | Executes both scripts, measures runtime, memory usage, LOC, maintainability, correctness, etc., and stores results in `results/evaluation.csv`. |
| âœ… **View results inside Streamlit**      | comparison table with Task ID, Model, Runtime, Memory, LOC, Maintainability Index, Errors, Output. |

You get a **complete side-by-side comparison** between Gemini 2.5 Flash and LLaMA-3.3-70B.

## ğŸ“ Folder Structure

```text
project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py                # Streamlit dashboard
â”‚   â”œâ”€â”€ orchestrator.py       # Generates code using both models
â”‚   â”œâ”€â”€ config.py             # Folder paths & settings
â”‚   â”œâ”€â”€ fetch_llm/
â”‚   â”‚    â””â”€â”€ call_gemini.py   # (and call_groq.py, etc.)
â”‚   â””â”€â”€ evaluator/
â”‚        â”œâ”€â”€ analyzer.py      # Runs evaluation
â”‚        â”œâ”€â”€ metrics.py       # Computes LOC, MI, etc.
â”‚        â””â”€â”€ runner.py        # Executes code safely
â”‚
â”œâ”€â”€ tasks/                    # Your task .txt files go here
â”œâ”€â”€ outputs/                  # Generated Python code (one subfolder per task)
â”œâ”€â”€ results/                  # evaluation.csv stored here
â””â”€â”€ README.md

| Metric                  | Description                                      |
|-------------------------|--------------------------------------------------|
| Runtime                 | Execution time (seconds)                         |
| Peak Memory Usage       | Maximum RAM consumed (MB)                        |
| Lines of Code (LOC)     | Total lines of generated code                    |
| Maintainability Index   | Standard MI score (higher = better)              |
| Raw Output              | Full stdout from execution                       |
```
## ğŸ§© How the Pipeline Works

This section explains how the entire code-generation and evaluation pipeline runs inside the project.

---

## 1. Create a Task

You can create tasks in two ways:

- Through the Streamlit UI  
- Or manually by adding a text file inside:

**Example task file content:**
Write a Python function to remove duplicates from a list.

## 2. Generate Code
Click the â€œGenerate Codeâ€ button.This runs: run_generation()

## 3. Run Evaluation
Click the â€œRun Evaluationâ€ button.This runs: run_evaluation()
Executes each generated Python file
Measures performance & quality metrics
Saves everything into results/evaluation.csv

### View Results
The Streamlit dashboard loads the CSV and displays:

Comparison table
Summary metrics
Execution outputs


